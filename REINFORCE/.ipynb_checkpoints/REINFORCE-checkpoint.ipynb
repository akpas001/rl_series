{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will implement REINFORCE algorithm for Discrete action spaces. The output of the network will be a probability distribution over actions.\n",
    "\n",
    "I used some parts of the following code and the RL course at Aalto University:\n",
    "https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma):\n",
    "    discounted_r = torch.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(range(0, r.size(-1))):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(torch.nn.Module):\n",
    "    def __init__(self, state_space, action_space):\n",
    "        super().__init__()\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.hidden = 64\n",
    "        self.fc1 = torch.nn.Linear(state_space, self.hidden)\n",
    "        self.out = torch.nn.Linear(self.hidden, action_space)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.out(x)\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "\n",
    "class Agent(object):\n",
    "    def __init__(self, policy):\n",
    "        self.train_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.policy = policy.to(self.train_device)\n",
    "        self.optimizer = torch.optim.RMSprop(policy.parameters(), lr=5e-3)\n",
    "        self.gamma = 0.98\n",
    "        self.states = []\n",
    "        self.action_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def episode_finished(self, episode_number):\n",
    "        action_probs = torch.stack(self.action_probs, dim=0) \\\n",
    "                .to(self.train_device).squeeze(-1)\n",
    "        rewards = torch.stack(self.rewards, dim=0).to(self.train_device).squeeze(-1)\n",
    "        self.states, self.action_probs, self.rewards = [], [], []\n",
    "\n",
    "        R = discount_rewards(rewards, self.gamma)\n",
    "        R = (R - R.mean()) / (R.std() + 1e-9) #normalize discounted rewards\n",
    "        loss = - (action_probs * R).mean()\n",
    "            \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def get_action(self, observation, evaluation=False):\n",
    "        x = torch.from_numpy(observation).float().to(self.train_device)\n",
    "        action_probs = self.policy(x)\n",
    "\n",
    "        if evaluation:\n",
    "            return torch.argmax(action_probs)\n",
    "        \n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        act_log_prob = dist.log_prob(action)\n",
    "        return action, act_log_prob\n",
    "\n",
    "    def store_outcome(self, observation, action_prob, reward):\n",
    "        self.states.append(observation)\n",
    "        self.action_probs.append(action_prob)\n",
    "        self.rewards.append(torch.Tensor([reward]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-11-09 13:52:29,555] Making new env: CartPole-v0\n",
      "/home/isaac/miniconda3/envs/pytorch13/lib/python3.6/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space_dim:  2\n",
      "observation_space_dim:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/miniconda3/envs/pytorch13/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished. Total reward: 19 (19 timesteps)\n",
      "False\n",
      "Episode 1 finished. Total reward: 51 (51 timesteps)\n",
      "False\n",
      "Episode 2 finished. Total reward: 30 (30 timesteps)\n",
      "False\n",
      "Episode 3 finished. Total reward: 89 (89 timesteps)\n",
      "False\n",
      "Episode 4 finished. Total reward: 74 (74 timesteps)\n",
      "False\n",
      "Episode 5 finished. Total reward: 95 (95 timesteps)\n",
      "False\n",
      "Episode 6 finished. Total reward: 62 (62 timesteps)\n",
      "False\n",
      "Episode 7 finished. Total reward: 26 (26 timesteps)\n",
      "False\n",
      "Episode 8 finished. Total reward: 41 (41 timesteps)\n",
      "False\n",
      "Episode 9 finished. Total reward: 50 (50 timesteps)\n",
      "False\n",
      "Episode 10 finished. Total reward: 41 (41 timesteps)\n",
      "False\n",
      "Episode 11 finished. Total reward: 50 (50 timesteps)\n",
      "False\n",
      "Episode 12 finished. Total reward: 108 (108 timesteps)\n",
      "False\n",
      "Episode 13 finished. Total reward: 130 (130 timesteps)\n",
      "False\n",
      "Episode 14 finished. Total reward: 83 (83 timesteps)\n",
      "False\n",
      "Episode 15 finished. Total reward: 200 (200 timesteps)\n",
      "False\n",
      "Episode 16 finished. Total reward: 121 (121 timesteps)\n",
      "False\n",
      "Episode 17 finished. Total reward: 51 (51 timesteps)\n",
      "False\n",
      "Episode 18 finished. Total reward: 65 (65 timesteps)\n",
      "False\n",
      "Episode 19 finished. Total reward: 36 (36 timesteps)\n",
      "False\n",
      "Episode 20 finished. Total reward: 128 (128 timesteps)\n",
      "False\n",
      "Episode 21 finished. Total reward: 61 (61 timesteps)\n",
      "False\n",
      "Episode 22 finished. Total reward: 89 (89 timesteps)\n",
      "False\n",
      "Episode 23 finished. Total reward: 65 (65 timesteps)\n",
      "False\n",
      "Episode 24 finished. Total reward: 72 (72 timesteps)\n",
      "False\n",
      "Episode 25 finished. Total reward: 57 (57 timesteps)\n",
      "False\n",
      "Episode 26 finished. Total reward: 99 (99 timesteps)\n",
      "False\n",
      "Episode 27 finished. Total reward: 26 (26 timesteps)\n",
      "False\n",
      "Episode 28 finished. Total reward: 40 (40 timesteps)\n",
      "False\n",
      "Episode 29 finished. Total reward: 47 (47 timesteps)\n",
      "False\n",
      "Episode 30 finished. Total reward: 45 (45 timesteps)\n",
      "False\n",
      "Episode 31 finished. Total reward: 30 (30 timesteps)\n",
      "False\n",
      "Episode 32 finished. Total reward: 110 (110 timesteps)\n",
      "False\n",
      "Episode 33 finished. Total reward: 83 (83 timesteps)\n",
      "False\n",
      "Episode 34 finished. Total reward: 87 (87 timesteps)\n",
      "False\n",
      "Episode 35 finished. Total reward: 56 (56 timesteps)\n",
      "False\n",
      "Episode 36 finished. Total reward: 78 (78 timesteps)\n",
      "False\n",
      "Episode 37 finished. Total reward: 99 (99 timesteps)\n",
      "False\n",
      "Episode 38 finished. Total reward: 35 (35 timesteps)\n",
      "False\n",
      "Episode 39 finished. Total reward: 107 (107 timesteps)\n",
      "False\n",
      "Episode 40 finished. Total reward: 112 (112 timesteps)\n",
      "False\n",
      "Episode 41 finished. Total reward: 56 (56 timesteps)\n",
      "False\n",
      "Episode 42 finished. Total reward: 114 (114 timesteps)\n",
      "False\n",
      "Episode 43 finished. Total reward: 116 (116 timesteps)\n",
      "False\n",
      "Episode 44 finished. Total reward: 116 (116 timesteps)\n",
      "False\n",
      "Episode 45 finished. Total reward: 200 (200 timesteps)\n",
      "False\n",
      "Episode 46 finished. Total reward: 89 (89 timesteps)\n",
      "False\n",
      "Episode 47 finished. Total reward: 116 (116 timesteps)\n",
      "False\n",
      "Episode 48 finished. Total reward: 113 (113 timesteps)\n",
      "False\n",
      "Episode 49 finished. Total reward: 177 (177 timesteps)\n",
      "False\n",
      "Episode 50 finished. Total reward: 150 (150 timesteps)\n",
      "False\n",
      "Episode 51 finished. Total reward: 78 (78 timesteps)\n",
      "False\n",
      "Episode 52 finished. Total reward: 92 (92 timesteps)\n",
      "False\n",
      "Episode 53 finished. Total reward: 200 (200 timesteps)\n",
      "False\n",
      "Episode 54 finished. Total reward: 105 (105 timesteps)\n",
      "False\n",
      "Episode 55 finished. Total reward: 80 (80 timesteps)\n",
      "False\n",
      "Episode 56 finished. Total reward: 200 (200 timesteps)\n",
      "False\n",
      "Episode 57 finished. Total reward: 83 (83 timesteps)\n",
      "False\n",
      "Episode 58 finished. Total reward: 43 (43 timesteps)\n",
      "False\n",
      "Episode 59 finished. Total reward: 195 (195 timesteps)\n",
      "False\n",
      "Episode 60 finished. Total reward: 187 (187 timesteps)\n",
      "False\n",
      "Episode 61 finished. Total reward: 200 (200 timesteps)\n",
      "False\n",
      "Episode 62 finished. Total reward: 200 (200 timesteps)\n",
      "False\n",
      "Episode 63 finished. Total reward: 200 (200 timesteps)\n",
      "False\n",
      "Episode 64 finished. Total reward: 167 (167 timesteps)\n",
      "False\n",
      "Episode 65 finished. Total reward: 110 (110 timesteps)\n",
      "False\n",
      "Episode 66 finished. Total reward: 128 (128 timesteps)\n",
      "False\n",
      "Episode 67 finished. Total reward: 142 (142 timesteps)\n",
      "False\n",
      "Episode 68 finished. Total reward: 200 (200 timesteps)\n",
      "False\n",
      "Episode 69 finished. Total reward: 98 (98 timesteps)\n",
      "False\n",
      "Episode 70 finished. Total reward: 189 (189 timesteps)\n",
      "False\n",
      "Episode 71 finished. Total reward: 122 (122 timesteps)\n",
      "False\n",
      "Episode 72 finished. Total reward: 132 (132 timesteps)\n",
      "False\n",
      "Episode 73 finished. Total reward: 124 (124 timesteps)\n",
      "False\n",
      "Episode 74 finished. Total reward: 181 (181 timesteps)\n",
      "False\n",
      "Episode 75 finished. Total reward: 192 (192 timesteps)\n",
      "False\n",
      "Episode 76 finished. Total reward: 62 (62 timesteps)\n",
      "False\n",
      "Episode 77 finished. Total reward: 119 (119 timesteps)\n",
      "False\n",
      "Episode 78 finished. Total reward: 113 (113 timesteps)\n",
      "False\n",
      "Episode 79 finished. Total reward: 117 (117 timesteps)\n",
      "False\n",
      "Episode 80 finished. Total reward: 92 (92 timesteps)\n",
      "False\n",
      "Episode 81 finished. Total reward: 93 (93 timesteps)\n",
      "False\n",
      "Episode 82 finished. Total reward: 167 (167 timesteps)\n",
      "False\n",
      "Episode 83 finished. Total reward: 199 (199 timesteps)\n",
      "False\n",
      "Episode 84 finished. Total reward: 157 (157 timesteps)\n",
      "False\n",
      "Episode 85 finished. Total reward: 55 (55 timesteps)\n",
      "False\n",
      "Episode 86 finished. Total reward: 166 (166 timesteps)\n",
      "False\n",
      "Episode 87 finished. Total reward: 68 (68 timesteps)\n",
      "False\n",
      "Episode 88 finished. Total reward: 200 (200 timesteps)\n",
      "False\n",
      "Episode 89 finished. Total reward: 162 (162 timesteps)\n",
      "False\n",
      "Episode 90 finished. Total reward: 200 (200 timesteps)\n",
      "False\n",
      "Episode 91 finished. Total reward: 124 (124 timesteps)\n",
      "False\n",
      "Episode 92 finished. Total reward: 200 (200 timesteps)\n",
      "False\n",
      "Episode 93 finished. Total reward: 200 (200 timesteps)\n",
      "False\n",
      "Episode 94 finished. Total reward: 200 (200 timesteps)\n",
      "False\n",
      "Episode 95 finished. Total reward: 145 (145 timesteps)\n",
      "False\n",
      "Episode 96 finished. Total reward: 126 (126 timesteps)\n",
      "False\n",
      "Episode 97 finished. Total reward: 200 (200 timesteps)\n",
      "False\n",
      "Episode 98 finished. Total reward: 200 (200 timesteps)\n",
      "False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f8f10550e85f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0maverage_reward_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mavg\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m195\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-d6b117a69c81>\u001b[0m in \u001b[0;36mepisode_finished\u001b[0;34m(self, episode_number)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch13/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pytorch13/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a Gym environment\n",
    "env_name = 'CartPole-v0'\n",
    "train_episodes = 5000\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Get dimensionalities of actions and observations\n",
    "action_space_dim = env.action_space.n\n",
    "observation_space_dim = env.observation_space.shape[-1]\n",
    "print('action_space_dim: ', action_space_dim)\n",
    "print('observation_space_dim: ', observation_space_dim)\n",
    "\n",
    "# Instantiate agent and its policy\n",
    "policy = Policy(observation_space_dim, action_space_dim)\n",
    "agent = Agent(policy)\n",
    "\n",
    "# Arrays to keep track of rewards\n",
    "reward_history, timestep_history = [], []\n",
    "average_reward_history = []\n",
    "\n",
    "# Run actual training\n",
    "for episode_number in range(train_episodes):\n",
    "    reward_sum, timesteps = 0, 0\n",
    "    done = False\n",
    "    # Reset the environment and observe the initial state\n",
    "    observation = env.reset()\n",
    "\n",
    "    # Loop until the episode is over\n",
    "    while not done:\n",
    "        # Get action from the agent\n",
    "        action, action_probabilities = agent.get_action(observation)\n",
    "        previous_observation = observation\n",
    "\n",
    "        # Perform the action on the environment, get new state and reward\n",
    "        observation, reward, done, info = env.step(action.detach().cpu().numpy())\n",
    "\n",
    "        # Store action's outcome (so that the agent can improve its policy)\n",
    "        agent.store_outcome(previous_observation, action_probabilities, reward)\n",
    "\n",
    "        # Store total episode reward\n",
    "        reward_sum += reward\n",
    "        timesteps += 1\n",
    "\n",
    "\n",
    "    print(\"Episode {} finished. Total reward: {:.3g} ({} timesteps)\"\n",
    "          .format(episode_number, reward_sum, timesteps))\n",
    "\n",
    "    reward_history.append(reward_sum)\n",
    "    timestep_history.append(timesteps)\n",
    "    if episode_number > 100:\n",
    "        avg = np.mean(reward_history[-100:])\n",
    "    else:\n",
    "        avg = np.mean(reward_history)\n",
    "    average_reward_history.append(avg)\n",
    "\n",
    "    agent.episode_finished(episode_number)\n",
    "    \n",
    "    if avg > 195:\n",
    "        print(f'Converged in episode {episode_number}')\n",
    "        break\n",
    "\n",
    "plt.plot(reward_history)\n",
    "plt.plot(average_reward_history)\n",
    "plt.legend([\"Reward\", \"100-episode average\"])\n",
    "plt.title(\"Reward history\")\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-11-09 13:31:30,679] Making new env: CartPole-v0\n",
      "/home/isaac/miniconda3/envs/pytorch13/lib/python3.6/site-packages/gym/envs/registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/isaac/miniconda3/envs/pytorch13/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test reward: 200.0 episode length: 200.0\n"
     ]
    }
   ],
   "source": [
    "render = True\n",
    "episodes = 10\n",
    "test_reward, test_len = 0, 0\n",
    "for ep in range(episodes):\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        # Similar to the training loop above -\n",
    "        # get the action, act on the environment, save total reward\n",
    "        # (evaluation=True makes the agent always return what it thinks to be\n",
    "        # the best action - there is no exploration at this point)\n",
    "#         print(observation)\n",
    "        action = agent.get_action(observation, evaluation=True)\n",
    "        observation, reward, done, info = env.step(action.detach().cpu().numpy())\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "        test_reward += reward\n",
    "        test_len += 1\n",
    "print(\"Average test reward:\", test_reward/episodes, \"episode length:\", test_len/episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
